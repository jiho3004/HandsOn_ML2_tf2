{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    #plt.title(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0] Data load\n",
    "패션 MNIST 데이터셋을 불러옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Load\n",
      "Train set:  (60000, 28, 28) [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
      " 180 212 210 211 213 223 220 243 202   0]\n",
      "Test set:  (10000, 28, 28) [  0   0   0   0   0   0   0   0   0   0   0   0   0   4   0  53 129 120\n",
      " 147 175 157 166 135 154 168 140   0   0]\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(img, y_train),(img_test, y_test) = fashion_mnist.load_data()\n",
    "print('* Load')\n",
    "print('Train set: ', img.shape, img[0][10])\n",
    "print('Test set: ', img_test.shape, img_test[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_image(img_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불러온 이미지를 prepocessing 함수를 통해 데이터셋으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(img, img_test):\n",
    "\n",
    "    # reshape\n",
    "    \n",
    "    x_train = np.reshape(img, (60000, 28,28,1))\n",
    "    x_test = np.reshape(img_test, (10000, 28,28,1))\n",
    "\n",
    "    print('* Flatten')\n",
    "    print(x_train.shape, x_train[0][300:328])\n",
    "\n",
    "    # Normalize\n",
    "    \n",
    "    x_train = x_train/255\n",
    "    x_test = x_test/255\n",
    "    \n",
    "    print('* Normailze')\n",
    "    print(x_train.shape, x_train[0][300:328])\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Flatten\n",
      "(60000, 28, 28, 1) []\n",
      "* Normailze\n",
      "(60000, 28, 28, 1) []\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = preProcessing(img, img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 14장 559쪽 Covlutional Neural Network (CNN)\n",
    "\n",
    "- 모델의 특징\n",
    "model_k : 첫번째 층은 64개의 큰 필터(7x7)와 스트라이드 1을 사용합니다. 이미지가 28x28 픽셀 크기이고 하나의 컬러 채널(그레이스케일)이므로\n",
    "          input_shape = [28, 28, 1]로 지정합니다.\n",
    "          \n",
    "          풀링 크기가 2인 최대 풀링 층을 추가하여 공간 방향 차원을 절반으로 줄입니다.\n",
    "          \n",
    "          CNN이 출력층에 다다를수록 필터 개수가 늘어나는데, 저수준 특성(예를 들면 작은 동심원, 수평선)의 개수는 적지만 이를 연결하여 고수준 특성을 만들 수 있는 방법이 많기 때문에 이런 구조가 합리적입니다. \n",
    "          \n",
    "          그 다음 Dense layer..두 개의 은닉층과 하나의 출력층으로 구성된 완전 연결 네트워크입니다. dense layer는 샘플의 특성으로 1차원 배열을 기대하므로, 입력을 일렬로 펼쳐야 합니다 (flatten이 그 역할을 합니다). 또 밀집 층 사이에 over fitting을 줄이기 위해 50% 드롭아웃 층을 추가합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_mnist_fashion():\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, 7, activation = \"relu\", padding = \"same\", input_shape = [28, 28, 1]),\n",
    "        tf.keras.layers.MaxPooling2D(2),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation = \"relu\", padding = \"same\"),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation = \"relu\", padding = \"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(2),\n",
    "        tf.keras.layers.Conv2D(256, 3, activation = \"relu\", padding = \"same\"),\n",
    "        tf.keras.layers.Conv2D(256, 3, activation = \"relu\", padding = \"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(64, activation = \"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation = \"softmax\")\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 183s 122ms/step - loss: 0.8010 - accuracy: 0.7132 - val_loss: 0.4024 - val_accuracy: 0.8532\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 186s 124ms/step - loss: 0.4540 - accuracy: 0.8453 - val_loss: 0.3339 - val_accuracy: 0.8844\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 202s 135ms/step - loss: 0.3852 - accuracy: 0.8668 - val_loss: 0.3042 - val_accuracy: 0.8882\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 208s 139ms/step - loss: 0.3469 - accuracy: 0.8800 - val_loss: 0.3232 - val_accuracy: 0.8747\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 209s 139ms/step - loss: 0.3148 - accuracy: 0.8896 - val_loss: 0.2762 - val_accuracy: 0.9024\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 215s 143ms/step - loss: 0.2968 - accuracy: 0.8977 - val_loss: 0.2811 - val_accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 209s 139ms/step - loss: 0.2782 - accuracy: 0.9039 - val_loss: 0.2792 - val_accuracy: 0.8982\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 218s 145ms/step - loss: 0.2633 - accuracy: 0.9091 - val_loss: 0.2592 - val_accuracy: 0.9063\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 207s 138ms/step - loss: 0.2500 - accuracy: 0.9137 - val_loss: 0.2885 - val_accuracy: 0.8997\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 206s 138ms/step - loss: 0.2448 - accuracy: 0.9146 - val_loss: 0.2621 - val_accuracy: 0.9112\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 204s 136ms/step - loss: 0.2332 - accuracy: 0.9196 - val_loss: 0.2711 - val_accuracy: 0.9137\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 203s 135ms/step - loss: 0.2169 - accuracy: 0.9237 - val_loss: 0.2628 - val_accuracy: 0.9104\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 207s 138ms/step - loss: 0.2156 - accuracy: 0.9266 - val_loss: 0.2864 - val_accuracy: 0.9137\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 204s 136ms/step - loss: 0.2010 - accuracy: 0.9308 - val_loss: 0.2779 - val_accuracy: 0.9084\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 210s 140ms/step - loss: 0.1973 - accuracy: 0.9314 - val_loss: 0.3006 - val_accuracy: 0.9077\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 209s 139ms/step - loss: 0.1941 - accuracy: 0.9337 - val_loss: 0.2646 - val_accuracy: 0.9165\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 217s 145ms/step - loss: 0.1862 - accuracy: 0.9352 - val_loss: 0.3001 - val_accuracy: 0.9071\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 215s 144ms/step - loss: 0.1809 - accuracy: 0.9391 - val_loss: 0.2831 - val_accuracy: 0.9147\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 215s 143ms/step - loss: 0.1794 - accuracy: 0.9387 - val_loss: 0.2941 - val_accuracy: 0.9096\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 221s 148ms/step - loss: 0.1659 - accuracy: 0.9427 - val_loss: 0.3200 - val_accuracy: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe24f5c84c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 생성\n",
    "model_k=make_model_mnist_fashion()\n",
    "\n",
    "# 모델 컴파일\n",
    "model_k.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 콜백 선언\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_k.h5', save_best_only=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model_k.fit(x_train, y_train, \n",
    "            validation_split=0.2,\n",
    "            epochs=20,\n",
    "            callbacks=[model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 모델 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 24ms/step - loss: 0.3627 - accuracy: 0.9097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3627490699291229, 0.9096999764442444]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_k.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
