{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11장\n",
    "### 11.4\n",
    "----\n",
    "\n",
    "### l1, l2, Dropout\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    #plt.title(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ 0 ] 데이터셋 준비\n",
    "\n",
    "* MNIST 데이터셋을 불러오세요. tf.keras.datasets.mnist.load_data()를 사용하여 데이터를 적재할 수 있습니다.\n",
    "* 데이터셋의 차원을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Load\n",
      "Train set:  (60000, 28, 28) [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "Test set:  (10000, 28, 28) [  0   0   0   0   0   0   0   0   0   0   0  17  66  14  67  67  67  59\n",
      "  21 236 254 106   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "(img, y_train), (img_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print('* Load')\n",
    "print('Train set: ', img.shape, img[0][10])\n",
    "print('Test set: ', img_test.shape, img_test[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_image(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 불러온 이미지을 **preProcessing** 함수를 통해 데이터셋으로 전환합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(img, img_test):\n",
    "\n",
    "    # reshape\n",
    "    \n",
    "    x_train = np.reshape(img, (60000, 28*28))\n",
    "    x_test = np.reshape(img_test, (10000, 28*28))\n",
    "\n",
    "    print('* Flatten')\n",
    "    print(x_train.shape, x_train[0][300:328])\n",
    "\n",
    "    # Normalize\n",
    "    \n",
    "    x_train = x_train/255\n",
    "    x_test = x_test/255\n",
    "    \n",
    "    print('* Normailze')\n",
    "    print(x_train.shape, x_train[0][300:328])\n",
    "    \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Flatten\n",
      "(60000, 784) [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 139 253 190   2   0   0   0   0   0]\n",
      "* Normailze\n",
      "(60000, 784) [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = preProcessing(img, img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# [ 1 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from functools import partial\n",
    "\n",
    "def make_model_1():\n",
    "    Dense = partial(layers.Dense,\n",
    "               activation='elu',\n",
    "               kernel_initializer='he_normal',\n",
    "               )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(28*28,)),\n",
    "        Dense(300),\n",
    "        Dense(100),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.summary() \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2418 - accuracy: 0.9259 - val_loss: 0.1320 - val_accuracy: 0.9605\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1056 - accuracy: 0.9671 - val_loss: 0.1156 - val_accuracy: 0.9666\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0762 - accuracy: 0.9758 - val_loss: 0.1116 - val_accuracy: 0.9649\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0559 - accuracy: 0.9815 - val_loss: 0.1002 - val_accuracy: 0.9722\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0441 - accuracy: 0.9856 - val_loss: 0.0899 - val_accuracy: 0.9756\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0344 - accuracy: 0.9888 - val_loss: 0.1054 - val_accuracy: 0.9754\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0314 - accuracy: 0.9895 - val_loss: 0.0974 - val_accuracy: 0.9767\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0245 - accuracy: 0.9915 - val_loss: 0.1091 - val_accuracy: 0.9775\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.1007 - val_accuracy: 0.9776\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0229 - accuracy: 0.9924 - val_loss: 0.1349 - val_accuracy: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9a7ec5dc0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 생성\n",
    "model_1=make_model_1()\n",
    "\n",
    "# 모델 컴파일\n",
    "model_1.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 콜백 선언\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_1.h5', save_best_only=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model_1.fit(x_train, y_train, \n",
    "          validation_split=0.2,\n",
    "          epochs=10,\n",
    "          callbacks=[model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# [ 2 ] l1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_2():\n",
    "    Dense = partial(layers.Dense,\n",
    "               activation='elu',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=tf.keras.regularizers.l1(0.01)\n",
    "               )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(28*28,)),\n",
    "        Dense(300),\n",
    "        Dense(100),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.summary() \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 콜백 ModelCheckpoint의 저장 이름을 **2_model.h5**로 바꿉니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 4.8280 - accuracy: 0.7745 - val_loss: 1.3990 - val_accuracy: 0.8436\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.3583 - accuracy: 0.8447 - val_loss: 1.2946 - val_accuracy: 0.8570\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.2779 - accuracy: 0.8585 - val_loss: 1.2423 - val_accuracy: 0.8670\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.2337 - accuracy: 0.8659 - val_loss: 1.2173 - val_accuracy: 0.8722\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.2030 - accuracy: 0.8687 - val_loss: 1.1588 - val_accuracy: 0.8734\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1757 - accuracy: 0.8702 - val_loss: 1.1151 - val_accuracy: 0.8873\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 1.1550 - accuracy: 0.8726 - val_loss: 1.1349 - val_accuracy: 0.8785\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1350 - accuracy: 0.8717 - val_loss: 1.0824 - val_accuracy: 0.8858\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1290 - accuracy: 0.8740 - val_loss: 1.0776 - val_accuracy: 0.8872\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1168 - accuracy: 0.8741 - val_loss: 1.0713 - val_accuracy: 0.8857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9c90f0310>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 생성\n",
    "model_2=make_model_2()\n",
    "\n",
    "# 모델 컴파일\n",
    "model_2.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 콜백 선언\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_2.h5', save_best_only=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model_2.fit(x_train, y_train, \n",
    "          validation_split=0.2,\n",
    "          epochs=10,\n",
    "          callbacks=[model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# [  3 ]  l2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_3():\n",
    "    Dense = partial(layers.Dense,\n",
    "               activation='elu',\n",
    "               kernel_initializer='he_normal',\n",
    "               kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "               )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(28*28,)),\n",
    "        Dense(300),\n",
    "        Dense(100),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.summary() \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 콜백 ModelCheckpoint의 저장 이름을 **3_model.h5**로 바꿉니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1.1047 - accuracy: 0.8739 - val_loss: 0.5693 - val_accuracy: 0.8979\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5528 - accuracy: 0.8988 - val_loss: 0.4802 - val_accuracy: 0.9218\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4928 - accuracy: 0.9123 - val_loss: 0.4789 - val_accuracy: 0.9090\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4582 - accuracy: 0.9200 - val_loss: 0.4285 - val_accuracy: 0.9260\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4448 - accuracy: 0.9224 - val_loss: 0.4091 - val_accuracy: 0.9352\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4287 - accuracy: 0.9248 - val_loss: 0.4316 - val_accuracy: 0.9252\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4285 - accuracy: 0.9255 - val_loss: 0.4223 - val_accuracy: 0.9253\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4108 - accuracy: 0.9281 - val_loss: 0.3623 - val_accuracy: 0.9444\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4164 - accuracy: 0.9276 - val_loss: 0.3728 - val_accuracy: 0.9384\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4044 - accuracy: 0.9294 - val_loss: 0.3770 - val_accuracy: 0.9388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9c9c72df0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 생성\n",
    "model_3=make_model_3()\n",
    "\n",
    "# 모델 컴파일\n",
    "model_3.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 콜백 선언\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_3.h5', save_best_only=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model_3.fit(x_train, y_train, \n",
    "          validation_split=0.2,\n",
    "          epochs=10,\n",
    "          callbacks=[model_checkpoint_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# [  4 ] DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_4():\n",
    "    Dense = partial(layers.Dense,\n",
    "               activation='elu',\n",
    "               kernel_initializer='he_normal'\n",
    "               )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(28*28,)),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        Dense(300),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        Dense(100),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.summary() \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 콜백 ModelCheckpoint의 저장 이름을 **4_model.h5**로 바꿉니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_6 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3662 - accuracy: 0.8870 - val_loss: 0.1558 - val_accuracy: 0.9541\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2082 - accuracy: 0.9352 - val_loss: 0.1114 - val_accuracy: 0.9658\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1632 - accuracy: 0.9496 - val_loss: 0.1116 - val_accuracy: 0.9668\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1402 - accuracy: 0.9557 - val_loss: 0.0925 - val_accuracy: 0.9693\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1225 - accuracy: 0.9620 - val_loss: 0.0886 - val_accuracy: 0.9714\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1120 - accuracy: 0.9648 - val_loss: 0.0838 - val_accuracy: 0.9743\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1040 - accuracy: 0.9671 - val_loss: 0.0770 - val_accuracy: 0.9754\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.0948 - accuracy: 0.9695 - val_loss: 0.0717 - val_accuracy: 0.9778\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.0912 - accuracy: 0.9715 - val_loss: 0.0739 - val_accuracy: 0.9788\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.0812 - accuracy: 0.9746 - val_loss: 0.0770 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9c9b7e580>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 생성\n",
    "model_4=make_model_4()\n",
    "\n",
    "# 모델 컴파일\n",
    "model_4.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 콜백 선언\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_4.h5', save_best_only=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model_4.fit(x_train, y_train, \n",
    "          validation_split=0.2,\n",
    "          epochs=10,\n",
    "          callbacks=[model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ 5 ]\n",
    "\n",
    "* 지금까지 만든 4가지 모델을 불러오고 평가해 봅니다.\n",
    "* **tf.keras.models.load_model( )**을 통해 모델을 불러올 수 있습니다.\n",
    "* **model.evaluate( )**를 통해 모든 모델을 평가해 봅니다.\n",
    "* 불러오기 전에 아래 셀에서 각 모델의 특징을 간단히 메모하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model_1:\n",
    "* model_2:\n",
    "* model_3:\n",
    "* model_4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 현재 tensorflow 버전에 로드 후 estimate할 때 버그가 있으므로 없애고 바로 평가 \n",
    "#model_1 = tf.keras.models.load_model('model_1.h5')\n",
    "#model_2 = tf.keras.models.load_model('model_2.h5')\n",
    "#model_3 = tf.keras.models.load_model('model_3.h5')\n",
    "#model_4 = tf.keras.models.load_model('model_4.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11001980304718018, 0.9775999784469604]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가\n",
    "\n",
    "model_1.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 1.0769 - accuracy: 0.8794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.076857089996338, 0.8794000148773193]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.9383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36992308497428894, 0.9383000135421753]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07298269122838974, 0.9786999821662903]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
